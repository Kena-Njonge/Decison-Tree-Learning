{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce049f5-4164-4d5f-89a2-93db67c6d7e5",
   "metadata": {},
   "source": [
    "# Decision Tree Learning\n",
    "\n",
    "When it comes to the problem of weak AI i.e. agents that act intelligent but are not conscious, there are many approaches to acheiving the goal of intellligence depending on the task. Some tasks such as (some) games \"only\" need clever algorithms at run-time as to acheive this. The algorithms may be very rudimentary in some cases. For example in tic-tac-toe, we wouldn't even have to use clever algorithms like minimax or alpha-beta search; one could easily hard code the best move for each state. Search algorithms are paricularly efficient for a wide range of applications,given good heuristic functions. Nevertheless, some cases call for another tool entirely, such as planning or learning. Today, we will take a look at the latter.\n",
    "\n",
    "\n",
    "Learning is the process of an agent improving its future performance by adapting its output based on a given percept sequence and knowledge about the world. Learning is an important tool as the perfect behavior for an agent may not be programmable. This can be due to a dynamic environment that prevents a programmer from implementing every possible action, e.g., the environment of a humanoid robot or the Mars exploration rovers, which can take steps of varying lengths. Or it may be due to a dynamic environment in which not every outcome is predictable, e.g., the stock market. Three forms of learning are common in artificial intelligence: supervised learning, unsupervised learning, and reinforcement learning. These types of learning differ in the form of feedback available for the agent. In supervised learning, both the input and output are available to the agent; reinforcement learning does not provide the output to the agent but provides feedback in the form of a reward or a punishment, and unsupervised learning does not provide any feedback. Decision Tree Learning is an inductive (supervised) form of learning where the agent learns a function based on given input-output pairs.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "I was introduced to decision tree learning as part of the \"Intro to AI\" course in the previous semester and had seen and executed the basic algorithm by hand. I was interested in learning how it was implemented in popular modules such as scikit-learn and gaining insight into some higher-level concepts for decision trees that I hadn't been introduced to yet.\n",
    "\n",
    "I completed a course on [Kaggle](https://www.kaggle.com/), \"Intro to Machine Learning,\" in which I was introduced to the DecisionTreeRegressor class and learned some basics of model evaluation based on data from the Iowa housing market. My main goal for this project was to improve my proficiency in pandas, matplotlib, scikit-learn, and practice using the core functions. I wanted to do some plotting and modeling and gain insight from it.\n",
    "\n",
    "As I continue learning, I may update this project or add other notebooks to this repository.\n",
    "\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This notebook uses the following dependencies. Please ensure you have Python installed. I used VS Code when writing down the execution instructions, so if you are not familiar with venvs, kindly download it; it provides a straightforward way to create them without having to use the command line. I will not rehash how to create a venv here. If you need a guide on that, please check the subsections \"Setting up a venv\", \"What is a venv\", and \"Creating a venv\" [here](https://github.com/Kena-Njonge/Grade_Analysis)\n",
    "\n",
    "The dependencies will be installed when you create the venv.\n",
    "\n",
    "- [Python](https://www.python.org/) (version 3.12.2 or later)\n",
    "- [Pandas](https://pandas.pydata.org/) (version 2.2.1 or later)\n",
    "- [Matplotlib](https://matplotlib.org/) (version 3.8.0 or later)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/) (version 1.3.0 or later)\n",
    "- [Ipython Kernel](https://ipython.org/) (version 6.29.3 or later)\n",
    "- [Numpy](https://numpy.org/) (version 1.26.4 or later)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Decion Trees.\n",
    "\n",
    "Decision Trees are a simple model, one of their main advantages is their explainability, as a user can clearly understand what steps were taken and what considerations there were in the generation of a certain output. They are also the basic building blocks for some more complicated models in data science, e.g., random forests. Decision Trees' main weakness is they are prone to overfitting; deep decision trees will capture noise and memorize data.\n",
    "\n",
    "\n",
    "There are two types of decision trees: classifiers and regression-based decision trees. The differentiator between the two is if the value that we want to predict, our target, and our features are all discrete or if some are continuous. If the values are discrete, we can use classification; otherwise, we use regression. Apart from this, the methods for creating a model are pretty similar. The model is trained by splitting the training data based on some attribute. We choose the attribute that either maximizes or minimizes a target value, depending on the context. This selection could be based on criteria such as the Gini coefficient, Information Gain, or Mean Squared Error. Despite their limitations, decision trees have shown themselves to be very useful in the industry.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "The dataset that I will use in this notebook was provided by Anaconda as part of their \"Data Analysis with Excel and Python course,\" which I completed. The course itself was mostly focused on showing that core Excel functionality such as pivot tables can just as easily be implemented in Python. For this, we used Pandas.\n",
    "\n",
    "The dataset consists of 392 entries with 8 attributes. Each entry consists of data about a car, e.g., its name, how many miles per gallon it achieves, the year it was built, and its origin. The dataset was provided with no NaN rows or other peculiarities, so there was no preprocessing needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eacefb-47f0-4b88-b4ed-a3d0cf6c033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import tree\n",
    "\n",
    "file_path = r\".\\auto_data.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57714a11-fbb5-4526-b0c4-7e7d8a54b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_list = list(df.columns)\n",
    "print(attribute_list)\n",
    "#The data has two unneeded columns, we delete these.\n",
    "df = df[attribute_list[0:9]]\n",
    "df['Year'] = df['yr'] + 1900 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed84de7-c07e-4f2f-918a-70f52899ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It never hurts to run a df.describe()\n",
    "df.describe()\n",
    "#Notice that non-numeric columns will (obviously) not be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fce21-b152-4a16-b889-21d113aff937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the relationship between mpg (miles per gallon) and the Year\n",
    "fig,ax = plt.subplots()\n",
    "X = df['Year']\n",
    "y = df['mpg']\n",
    "ax.scatter(X,y)\n",
    "ax.set_ylabel(\"mpg\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12bc9c-b464-46ac-9baa-ad5bf87243f6",
   "metadata": {},
   "source": [
    "Visually, one can observe a general upward trend in miles per gallon (mpg) as the years increase, indicating that newer cars tend to be more energy efficient. Additionally, there is noticeable variance in mpg across different years. To analyze this further, let's group our values into bins and quantify the spread per year. Instead of using prebuilt modules, we'll take a naive approach. But before we do that, let's formalize and quantify the correlation that we observe and determine the regression line, also known as the line of best fit.\n",
    "\n",
    "\n",
    "The correlation coefficient used in numpy is the Pearson correlation coefficient. Numpy utilitzes least square method to determine the regression line. If you want more information on how to determine the regression line, or if you're interested in implementing it yourself without external libraries, you can refer to [this article](https://www.varsitytutors.com/hotmath/hotmath_help/topics/line-of-best-fit) by Varsity Tutors where they go over the calculation in depth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9118f9-9cbb-4ecd-8da5-53b58a90654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "X = df['Year']\n",
    "y = df['mpg']\n",
    "ax.scatter(X,y)\n",
    "\n",
    "corr_coeff = np.corrcoef(X,y)\n",
    "\n",
    "#The parameter 1 specifies the degree of the polynomial to fit.\n",
    "slope, intercept = np.polyfit(X,y,1)\n",
    "\n",
    "regression_line = slope * X + intercept\n",
    "plt.plot(X, regression_line, color='red', label='Regression_Line')\n",
    "\n",
    "plt.title('Scatter Plot with Regression Line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('mpg')\n",
    "plt.show()\n",
    "print(f\"Our correlation coefficient is {corr_coeff[0][1]} . We have a positive correlation between 'mpg' and 'year'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78112116-3fce-424e-9fb2-df2cfee1c501",
   "metadata": {},
   "source": [
    "Using gradient descent can potentially yield an even better regression line. Gradient descent is a fundamental algorithm in machine learning. If you're interested in learning more about the algorithm and understanding it from a mathematical standpoint, [this medium article](https://medium.com/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4) helped me understand it better.\n",
    "\n",
    "Although I won't be covering gradient descent in this notebook, I may do so at a later date. What I will demonstrate next is that you can achieve the same regression line using the LinearRegression class in sklearn. In my last notebook, I didn't plot the regression line; instead, I plotted the predictions. While this approach isn't necessarily incorrect, it might have caused some confusion, as readers generally expect to see a line when the author discusses linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dda801-35d7-4715-803e-36cfe617864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Creating the test and train sets and creating our model, \n",
    "# We did not plot in the predictions this time.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "X_train = X_train.values.reshape(-1,1)\n",
    "lin_reg.fit(X.values.reshape(-1,1), y)\n",
    "y_pred_lin = lin_reg.predict(X_test.values.reshape(-1,1))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X,y)\n",
    "\n",
    "# Plotting the regression line\n",
    "slope_2 = lin_reg.coef_\n",
    "intercept_2 = lin_reg.intercept_\n",
    "regression_line_2 = intercept_2 + slope_2 * X\n",
    "\n",
    "plt.plot(X, regression_line_2, color='blue', label='Regression_Line ')\n",
    "\n",
    "\n",
    "plt.title('Scatter Plot with Regression Line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec35f48-3381-40b9-8c25-3caa4a1c8174",
   "metadata": {},
   "source": [
    "One can observe a significant amount of variance in the miles per gallon achieved by cars produced in different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96321ec3-2d6f-4ea7-9d9e-a0426987e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use series.uniques as to quickly count the number of years\n",
    "unique_years = df['Year'].unique()\n",
    "\n",
    "fig1, axes = plt.subplots(1,len(unique_years), figsize=(20,5))\n",
    "#If you want to do something with the index of a list element, or iterable but also need to iterate over the elements themselves \n",
    "# use the enumerate function\n",
    "standard_deviation = dict()\n",
    "for year_index, year in enumerate(unique_years):\n",
    "        year_df = df[df['Year']==year]\n",
    "        standard_deviation[year] = year_df.describe().loc['std','mpg']\n",
    "        axes[year_index].boxplot(year_df['mpg'])\n",
    "        #Use an f-string a (formatted literal string) to quickly deal with the multiple titles\n",
    "        axes[year_index].set_title(f\"mpg in {year}\")\n",
    "plt.show()\n",
    "print(\"The standard deviations are as follows.\")\n",
    "print(standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f117b-5c14-4b08-8504-e97a242f0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deviation = max(standard_deviation.values())\n",
    "min_deviation = min(standard_deviation.values())\n",
    "\n",
    "#It is smarter to iterate over items, the (key,value) pair, as I need to access both the key and the value \n",
    "for key, value in standard_deviation.items():\n",
    "    if value==max_deviation:\n",
    "        print(f\"The maximum standard deviation was {max_deviation} in the year {key}\")\n",
    "    if value==min_deviation:\n",
    "        print(f\"The minimum standard deviation was {min_deviation} in the year {key}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd183fc-ceda-4a49-bc5a-ce2352443a34",
   "metadata": {},
   "source": [
    "We can observe that the year with the largest variance in mpg is 1978. This variance, which represents the spread or dispersion of the mpg values around the mean, is calculated as the square of the standard deviation. So it follows from our calculation of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4dfff-ed15-4907-bfaf-239999eb61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_deviations = sorted(standard_deviation.items(), key=lambda item: item[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# I will not lie, I still do not understand why I constantly have to reshape some arrays\n",
    "\n",
    "X_year_var = np.array([sub_array[0] for sub_array in sorted_deviations]).reshape(-1, 1)\n",
    "y_year_var = np.array([sub_array[1] for sub_array in sorted_deviations]).reshape(-1, 1)\n",
    "\n",
    "ax.scatter(X_year_var, y_year_var)\n",
    "ax.set_xlabel(\"year\")\n",
    "ax.set_ylabel(\"standard deviation of mpg\")\n",
    "\n",
    "var_reg = LinearRegression()\n",
    "var_reg.fit(X_year_var, y_year_var)\n",
    "\n",
    "var_intercept = var_reg.intercept_\n",
    "var_slope = var_reg.coef_\n",
    "\n",
    "#Just as I thought for some reason this method does not work if I reshape the data haha\n",
    "#Plotting the regression line. \n",
    "corr_coef_var = np.corrcoef([sub_array[0] for sub_array in sorted_deviations],[sub_array[1] for sub_array in sorted_deviations])\n",
    "var_reg_func = var_intercept + var_slope * X_year_var\n",
    "ax.plot(X_year_var, var_reg_func, color='red', label=\"Regression Line\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Our correlation coefficient is {corr_coef_var[0][1]} . We have a positive correlation between 'standard deviation of mpg' and 'year'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c658110-af06-47ea-82df-e679a903573c",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "Now that we have a solid understanding of what the dataset looks like, along with some correlations and characteristics, we can proceed with creating a model. As discussed in my previous notebook, to build an effective model, we need to evaluate its performance. For this, we must split the data into training and test subsets. Scikit-learn provides a function for this in the `model_selection` submodule, typically splitting the data in a 3:1 ratio by default.\n",
    "\n",
    "Before proceeding, we need to define our features (the attributes we use for prediction) and our prediction target. There's generally no fixed rule for determining the optimal number of features for a given target variable.\n",
    "\n",
    "We'll designate our target variable as \"mpg\" and consider the other columns as our features. In data analysis, two types of decision trees are commonly used: classification trees and regression trees. Regression tree analysis is employed when the predicted outcome can be considered a real number. As expanded upon in Russel and Norvig's book \"Artificial Intelligence: A Modern Approach\": \"A regression tree has at each leaf a linear function of some subset of numerical attributes, rather than a single value. For example, the branch for two-bedroom apartments might end with a linear function of square footage, number of bathrooms, and average income for the neighborhood.\" Given that we are dealing with continuous values, we'll opt for a regression tree.\n",
    "\n",
    "Before proceeding with model creation, we need to transform non-numeric items, such as car names and their origins, into numeric equivalents. Creating a mapping from origin to a number is straightforward since we only have three values, allowing us to hard-code the mapping. However, mapping names to numbers is a bit more involved. We'll store the mapped values in `origin_id` and `name_id` respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72ae8f-d51b-4e78-ac8b-9c6306b61b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['origin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf479cb-e1ce-4893-a5c1-46f0e3cb2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(df['name'])\n",
    "\n",
    "name_dict = defaultdict(list)\n",
    "\n",
    "#We need to classify all the names, my approach is first to populate a dict with the different names,\n",
    "#then transform it into a list and have our mapping be based on the list index\n",
    "#I now know a more efficient way to implement this, but I leave that task to you \n",
    "#Hint: You can just use the .unique() method\n",
    "for index, name in enumerate(names):\n",
    "    name_dict[name].append(index)\n",
    "\n",
    "#Create new columns\n",
    "df['name_id'] = 0\n",
    "df['origin_id'] = 0\n",
    "\n",
    "\n",
    "fetch_first_elements = lambda arr: [subarr[0] for subarr in arr]\n",
    "\n",
    "#We used this to get all the car names after classifying them in the first step. \n",
    "#This is part of the suboptimal initial approach\n",
    "name_list = fetch_first_elements(list(name_dict.items()))\n",
    "\n",
    "\n",
    "#This is the mapping portion\n",
    "for index in df.index:\n",
    "    df.loc[index,'name_id'] = name_list.index(df.loc[index,'name'])\n",
    "    if df.loc[index,'origin'] == 'US':\n",
    "        df.loc[index,'origin_id'] = 0\n",
    "    elif df.loc[index,'origin'] == 'Asia':\n",
    "        df.loc[index,'origin_id'] = 1\n",
    "    else:\n",
    "        #For Europe. All entries in our df have an origin, so this is possible\n",
    "         df.loc[index,'origin_id'] = 2\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f395eee-6980-4c5b-b8eb-633347b2413d",
   "metadata": {},
   "source": [
    "We do a final data formatting step. Dropping the original 'yr' column as we do it need it anymmore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ae5bf-8fbb-4005-938f-b9eb9146d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = df.drop(\"yr\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d920b-1981-4cbc-aec9-ec1341a4d9b1",
   "metadata": {},
   "source": [
    "We are now ready to create the model. We split the data into training and test sets and then train multiple decision trees with different depths. Next, we evaluate their performance using the test data based on the mean absolute error of each one. We choose the best-performing model and display it. Additionally, the decision tree will be saved in the current folder, allowing for better viewing of its nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17906027-662d-403f-8c94-7472aaf77f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = df[['cyl', 'displ', 'hp', 'weight', 'accel', 'origin_id', 'name_id',\n",
    "       'Year']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,y,random_state=42)\n",
    "\n",
    "# We select a couple of depths to test.\n",
    "# As far as I understand, it doesn't make any sense to have a higher depth than we have features\n",
    "depths = [4, 5, 6, 7, 8]\n",
    "\n",
    "# Training decision trees with different depths and evaluating performance\n",
    "mean_abs_errors = []\n",
    "for depth in depths:\n",
    "    mpg_model = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    mpg_model.fit(X_train, y_train)\n",
    "    y_prediction = mpg_model.predict(X_test)\n",
    "    mean_abs_errors.append(mean_absolute_error(y_test, y_prediction))\n",
    "\n",
    "# Find the best depth based on mean absolute errors\n",
    "best_depth = depths[mean_abs_errors.index(min(mean_abs_errors))]\n",
    "\n",
    "print(f\"The decision tree performs best with a depth of {best_depth}\")\n",
    "print(f\"The mean absolute errors of the different depths are: {mean_abs_errors}\")\n",
    "\n",
    "# Plotting the decision tree\n",
    "fig, ax = plt.subplots(figsize=(70, 50))\n",
    "tree.plot_tree(mpg_model, ax=ax, filled=True, feature_names=list(X_1.columns))  \n",
    "\n",
    "# Saving the plot if it doesn't already exist\n",
    "if not os.path.exists(\"Decision_Tree.png\"):\n",
    "    plt.savefig(\"Decision_Tree.png\")\n",
    "    print(\"Decision tree visualization saved as 'Decision_Tree.png'\")\n",
    "else:\n",
    "    print(\"File 'Decision_Tree.png' already exists\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f5e74-6a72-4514-97f0-646bae6093f2",
   "metadata": {},
   "source": [
    "## Final Outlook and areas for improvement.\n",
    "\n",
    "Overall, I am quite satisfied with what I was able to create. I had a lot of fun going through this modeling process and learning new modules, as well as sharpening my skills with known ones. In hindsight, I think similar model quality could have been achieved with fewer features. To achieve this, I should have first examined if there is a correlation between the other variables and mpg. If the correlation is minor compared to the others, then it would be fair to assume that these do not bring us much more information when our goal is calculating the mpg. I am also a little disappointed in the depth of the tree; we are bordering on overfitting as we have 8 possible features, and we use 7 in our decision tree. The biggest improvement that I think I could make on my next go is using a random forest as my model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
