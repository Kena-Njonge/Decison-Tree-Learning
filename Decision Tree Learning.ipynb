{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bce049f5-4164-4d5f-89a2-93db67c6d7e5",
   "metadata": {},
   "source": [
    "# Decision Tree Learning\n",
    "\n",
    "Learning is the process of an agent improving its future performance by adapting its output based on a given percept sequence and knowledge about the world. Learning is an important tool as the perfect behavior for an agent may not be programmable. This can be due to a dynamic environment that prevents a programmer from implementing every possible action e.g. the environment of a humanoid robot or the Mars exploration rovers, which can take a step of 30cm or 20 cm etc., or a dynamic environment in which not every outcome is predictable e.g. the stock market. Three forms of learning are common when it comes to artificial intelligence, supervised learning, unsupervised learning, and reinforcement learning. These types of learning differ in the form of feedback that is available for the agent. When it comes to supervised learning both the input and output are available to the agent, reinforcement learning does not provide the output to the agent but provides feedback in the form of a reward or a punishment, and unsupervised learning does not provide any feedback. Decision Tree Learning is an inductive (supervised) form of learning where the agent learns a function based on given input-output pairs. \n",
    "\n",
    "## Motivation\n",
    "\n",
    "I got an introduction to decision tree learning as part of the \"Intro to AI\" course in the previous semester and had seen and executed the basic algorithm by hand, I was interested in learning how it was implemented in popular modules such as scikit-kearn and gaining insight into some higher level concepts for decision trees that I hadn't been introduced to yet. \n",
    "\n",
    "I completed a course on Kaggle \"Intro to Machine Learning\" in which I was introduced to the DecisionTreeRegressor class and learned some basics of model evaluation based on data from the Iowa housing market. As I learn more, I may update this project, or add other notebooks to this repository.  \n",
    "\n",
    "\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This notebook uses the following. Please ensure you have Python and Java installed. I used VS code when writing down the execution instructions, so if you are not familiar with venvs kindly download it, it provides a very easy way to create them without having to use the command line. I will not rehash how to create a venv here. If you need a guide on that, please check the subsections \"Setting up a venv\", \"What is a venv\" and \"Creating a venv\" [here](https://github.com/Kena-Njonge/Grade_Analysis)\n",
    "\n",
    "The dependencies will be automatically installed when you create the venv.\n",
    "\n",
    "- [Python](https://www.python.org/) (version 3.12.2 or later)\n",
    "- [Pandas](https://pandas.pydata.org/) (version 2.2.1 or later)\n",
    "- [Matplotlib](https://matplotlib.org/) (version 3.8.0 or later)\n",
    "- [scikit-learn](https://scikit-learn.org/stable/) (version 1.3.0 or later)\n",
    "- [Ipython Kernel](https://ipython.org/) (version 6.29.3 or later)\n",
    "- [Numpy](https://numpy.org/) (version 1.26.4 or later)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Decion Trees.\n",
    "\n",
    "Decision Trees are a simple model, one of their main advantages is their explainability as a user can clearly understand what steps were taken and what considerations there were in the generation of a certain output. They are also the basic building blocks for some more complicated models in data science e.g. random forests. Decision Trees are prone to overfitting and deep decision trees will capture noise and memorize data. \n",
    "\n",
    "\n",
    "There are two types of decision trees, classifiers and regression-based decision trees. The differentiator between the two is if the value that we want to predict, our target, and our features are all discrete or if some are continuous. Apart from this the methods for creating a model are pretty similar. The model is trained by splitting the training data based on some (discrete or continuous) attribute. We choose the attribute that maximizes respectively minimizes a target value, this could be the Gini coefficient, the Information Gain, or the Mean Squared Error. Once the training is complete, we can use our decision tree to predict future outcomes. Despite their limitations, decision trees have shown themselves to be very useful in industry. \n",
    "## The dataset\n",
    "\n",
    "The dataset that I will use in this notebook was provided by Anaconda as part of their \"Data Analysis with Excel and Python course\" which I completed. The course itself was mostly focused on showing that core Excel functionality such as pivot tables, can just as easily be implemented in Python. For this we used pandas.  Kaggle is a platform that hosts competitions where data scientists and machine learning enthusiasts can compete to solve real-world problems using machine learning techniques. Additionally, they provide real-world datasets and courses to improve one's data science knowledge and skills. \n",
    "\n",
    "We will take a look at data on the Melbourne housing market. The following data set is a subset of the dataset created by Tony Pino. He created it by scraping data from listings on domain.com.au. The data was cleaned by Tony Pino and includes multiple attributes for each house e.g. rooms, price, method, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eacefb-47f0-4b88-b4ed-a3d0cf6c033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import os\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import tree\n",
    "\n",
    "#src= r\"C:\\Users\\kenak\\Documents\\Blog\\Python for Spreadsheets\\auto_data.xlsx\"\n",
    "#dst=r\".\\auto_data.xlsx\"\n",
    "\n",
    "# I didn't want to copy the file, so I created a symlink. Remember to change this later when shipping the code. You need privileges to create a symlink, so I am just going to copy the file.\n",
    "# os.symlink(src, dst)\n",
    "file_path = r\".\\auto_data.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "#The data had two unneeded columns, we deleted these.\n",
    "\n",
    "#iloc gives us the rows with those indexes, not the columns, so this approach didn't work\n",
    "# df=df.iloc[0:6]\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57714a11-fbb5-4526-b0c4-7e7d8a54b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_list = list(df.columns)\n",
    "#Remember the [] notation for lists does not include the last element.\n",
    "print(attribute_list)\n",
    "df = df[attribute_list[0:9]]\n",
    "df['Year'] = df['yr'] + 1900 \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed84de7-c07e-4f2f-918a-70f52899ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#It never hurts to run a df.describe()\n",
    "df.describe()\n",
    "#Notice that non-numeric columns will (obviously) not be included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080fce21-b152-4a16-b889-21d113aff937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "X = df['Year']\n",
    "y = df['mpg']\n",
    "ax.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f12bc9c-b464-46ac-9baa-ad5bf87243f6",
   "metadata": {},
   "source": [
    "Visually one can observe a general trend upward in mpg as the years rise, so the newer the car, the more energy efficient it is. We can also observe, that there is a lot of variance in the mpg. Let us group our values into bins and quantify the spread per year. We will use a naive approach instead of using prebuilt modules. Before we do that, let us formalize and quantify the correlation that we see in a function and determine the regression line (also known as the line of best fit).\n",
    "\n",
    "\n",
    "\n",
    "The correlation coefficient used in numpy is the Pearson correlation coefficient. Numpy uses the least square method to determine the regression line. If you want more information on how to determine the regression line, or would like to know how to do this without external libraries i.e. you are interested in implementing it yourself, then you can take a look at [this](https://www.varsitytutors.com/hotmath/hotmath_help/topics/line-of-best-fit) article by Varsity Tutors where they go over the calculation in depth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9118f9-9cbb-4ecd-8da5-53b58a90654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "X = df['Year']\n",
    "y = df['mpg']\n",
    "ax.scatter(X,y)\n",
    "\n",
    "corr_coeff = np.corrcoef(X,y)\n",
    "\n",
    "#The parameter 1 specifies the degree of the polynomial to fit.\n",
    "slope, intercept = np.polyfit(X,y,1)\n",
    "\n",
    "regression_line = slope * X + intercept\n",
    "plt.plot(X, regression_line, color='red', label='Regression_Line')\n",
    "\n",
    "plt.title('Scatter Plot with Regression Line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('mpg')\n",
    "plt.show()\n",
    "print(f\"Our correlation coefficient is {corr_coeff[0][1]} . We have a positive correlation between 'mpg' and 'year'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78112116-3fce-424e-9fb2-df2cfee1c501",
   "metadata": {},
   "source": [
    "Using gradient descent one can get an even better regression line if possible. Gradient descent is a fundamental algorithm in machine learning, if you would like to learn more about the algorithm and have it explained from a mathematical standpoint, [this](https://medium.com/analytics-vidhya/linear-regression-with-gradient-descent-derivation-c10685ddf0f4) medium article helped me understand it better.\n",
    "\n",
    "I won't be getting into it in this notebook, I may do so at a later date. What I will demonstrate next, is that one can get the same regression line by using the LinearRegression class in sklearn. In my last notebook, I didn't plot the regression line, I instead plotted the predictions. This isn't necessarily wrong as far as I understand, I just used the model to predict the values, but it may have been a cause of confusion as generally, one may expect some sort of line when the author talks about a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dda801-35d7-4715-803e-36cfe617864a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42)\n",
    "X_train = X_train.values.reshape(-1,1)\n",
    "lin_reg.fit(X.values.reshape(-1,1), y)\n",
    "y_pred_lin = lin_reg.predict(X_test.values.reshape(-1,1))\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X,y)\n",
    "\n",
    "slope_2 = lin_reg.coef_\n",
    "intercept_2 = lin_reg.intercept_\n",
    "regression_line_2 = intercept_2 + slope_2 * X\n",
    "\n",
    "plt.plot(X, regression_line_2, color='blue', label='Regression_Line ')\n",
    "\n",
    "\n",
    "plt.title('Scatter Plot with Regression Line')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec35f48-3381-40b9-8c25-3caa4a1c8174",
   "metadata": {},
   "source": [
    "One can see that there is a great deal of variance in the miles per gallon that a car would acheive given that it was produced in a certain year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96321ec3-2d6f-4ea7-9d9e-a0426987e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use series.uniques as to quickly count the number of years\n",
    "unique_years = df['Year'].unique()\n",
    "\n",
    "fig1, axes = plt.subplots(1,len(unique_years), figsize=(20,5))\n",
    "#If you want to do something with the index of a list element, or iterable but also need to iterate over the elements themselves \n",
    "# use the enumerate function\n",
    "standard_deviation = dict()\n",
    "for year_index, year in enumerate(unique_years):\n",
    "        year_df = df[df['Year']==year]\n",
    "        standard_deviation[year] = year_df.describe().loc['std','mpg']\n",
    "        axes[year_index].boxplot(year_df['mpg'])\n",
    "        #Use an f-string a (formatted literal string) to quickly deal with the multiple titles\n",
    "        axes[year_index].set_title(f\"mpg in {year}\")\n",
    "plt.show()\n",
    "print(\"The standard deviations are as follows.\")\n",
    "print(standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f117b-5c14-4b08-8504-e97a242f0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_deviation = max(standard_deviation.values())\n",
    "min_deviation = min(standard_deviation.values())\n",
    "\n",
    "#It is smarter to iterate over items, the (key,value) pair, as I need to access both the key and the value \n",
    "for key, value in standard_deviation.items():\n",
    "    if value==max_deviation:\n",
    "        print(f\"The maximum standard deviation was {max_deviation} in the year {key}\")\n",
    "    if value==min_deviation:\n",
    "        print(f\"The minimum standard deviation was {min_deviation} in the year {key}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd183fc-ceda-4a49-bc5a-ce2352443a34",
   "metadata": {},
   "source": [
    "We can see that the years with the largest variance in mpg is 1978. The variance is just the square of the standard deviation, so it follows from our calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4dfff-ed15-4907-bfaf-239999eb61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_deviations = sorted(standard_deviation.items(), key=lambda item: item[1])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# I will not lie, I still do not understand why I constantly have to reshape some arrays\n",
    "\n",
    "X_year_var = np.array([sub_array[0] for sub_array in sorted_deviations]).reshape(-1, 1)\n",
    "y_year_var = np.array([sub_array[1] for sub_array in sorted_deviations]).reshape(-1, 1)\n",
    "\n",
    "ax.scatter(X_year_var, y_year_var)\n",
    "ax.set_xlabel(\"year\")\n",
    "ax.set_ylabel(\"standard deviation of mpg\")\n",
    "\n",
    "var_reg = LinearRegression()\n",
    "var_reg.fit(X_year_var, y_year_var)\n",
    "\n",
    "var_intercept = var_reg.intercept_\n",
    "var_slope = var_reg.coef_\n",
    "\n",
    "#Just as I thought for some reason this method does not work if I reshape the data haha\n",
    "#Plotting the regression line. \n",
    "corr_coef_var = np.corrcoef([sub_array[0] for sub_array in sorted_deviations],[sub_array[1] for sub_array in sorted_deviations])\n",
    "var_reg_func = var_intercept + var_slope * X_year_var\n",
    "ax.plot(X_year_var, var_reg_func, color='red', label=\"Regression Line\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Our correlation coefficient is {corr_coef_var[0][1]} . We have a positive correlation between 'standard deviation of mpg' and 'year'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c658110-af06-47ea-82df-e679a903573c",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "Now that we have gotten a firm grasp on what the data set looks like as well as some correlations and characteristics, we can proceed with creating a model. As discussed in my previous notebook, to create an expressive model, we need to evaluate the model's performance. For this, we need to split the data into training and test subsets. scikit-learn provides a function for this in the model_selection submodule. By default, the split is 3:1. \n",
    "\n",
    "Before doing this, we have to define our features (the attributes that we use when making a prediction) and our prediction target. There is generally no way to determine the amount of features that are ideal for a given target variable.\n",
    "\n",
    "We will let our target variable be mpg and the other columns be our features. Two types of decision trees are used in data analysis, classification trees and regression trees. Regression tree analysis is used when the predicted outcome can be considered a real number. As expanded upon in Russel and Norvig's book \"Artificial Intelligence: A Modern Approach\": \"A regression tree has at each leaf a linear function of some subset of numerical attributes, rather than a single value. For example, the branch for two-\n",
    "bedroom apartments might end with a linear function o  square footage, numbero  \n",
    "bathrooms, anthe d average income for the neighbooodrh\". /////As we are working witcontinuouste values we will use regressionot Tree.\n",
    "\n",
    "The final thing that we need to do before we can proceed with creating our model is transfong non-numeric items, in this ca,sthe carun Names antheir originrs into numeric for Creating a mapping from origin to a number is pretty simple as we only have three values, we can thus hard code the mapping. The mapping of names to numbers was a little more involved. We stored the mapped values in origin_id and name_id respectively.d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72ae8f-d51b-4e78-ac8b-9c6306b61b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['origin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf479cb-e1ce-4893-a5c1-46f0e3cb2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(df['name'])\n",
    "\n",
    "name_dict = defaultdict(list)\n",
    "\n",
    "#We need to classify all the names, my approach is first to populate a dict with the different names,\n",
    "#then transform it into a list and have our mapping be based on the list index\n",
    "#I now know a more efficient way to implement this, but I leave that task to you \n",
    "#Hint: You can just use the .unique() method\n",
    "for index, name in enumerate(names):\n",
    "    name_dict[name].append(index)\n",
    "\n",
    "#Create new columns\n",
    "df['name_id'] = 0\n",
    "df['origin_id'] = 0\n",
    "\n",
    "\n",
    "fetch_first_elements = lambda arr: [subarr[0] for subarr in arr]\n",
    "\n",
    "#We used this to get all the car names after classifying them in the first step. \n",
    "#This is part of the suboptimal initial approach\n",
    "name_list = fetch_first_elements(list(name_dict.items()))\n",
    "\n",
    "\n",
    "#This is the mapping portion\n",
    "for index in df.index:\n",
    "    df.loc[index,'name_id'] = name_list.index(df.loc[index,'name'])\n",
    "    if df.loc[index,'origin'] == 'US':\n",
    "        df.loc[index,'origin_id'] = 0\n",
    "    elif df.loc[index,'origin'] == 'Asia':\n",
    "        df.loc[index,'origin_id'] = 1\n",
    "    else:\n",
    "        #For Europe. All entries in our df have an origin, so this is possible\n",
    "         df.loc[index,'origin_id'] = 2\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f395eee-6980-4c5b-b8eb-633347b2413d",
   "metadata": {},
   "source": [
    "We do a final data formatting step. Dropping the original 'yr' column as we do it need it anymmore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ae5bf-8fbb-4005-938f-b9eb9146d5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = df.drop(\"yr\", axis=1)\n",
    "except:\n",
    "    pass\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d920b-1981-4cbc-aec9-ec1341a4d9b1",
   "metadata": {},
   "source": [
    "We are now ready to create the model. We split the data into training and test sets and then train multiple decision trees with different depths. We then evaluate their performance using the test data based on the mean absolute error of each one. We choose the best-performing model and display it. The decision tree will also be saved in the current folder, as one can zoom in and observe the intricacies better that way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17906027-662d-403f-8c94-7472aaf77f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_1 = df[['cyl', 'displ', 'hp', 'weight', 'accel', 'origin_id', 'name_id',\n",
    "       'Year']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1,y,random_state=42)\n",
    "\n",
    "# We select a couple of depths to test.\n",
    "# As far as I understand, it doesn't make any sense to have a higher depth than we have features\n",
    "depths = [4, 5, 6, 7, 8]\n",
    "\n",
    "# Training decision trees with different depths and evaluating performance\n",
    "mean_abs_errors = []\n",
    "for depth in depths:\n",
    "    mpg_model = DecisionTreeRegressor(max_depth=depth, random_state=42)\n",
    "    mpg_model.fit(X_train, y_train)\n",
    "    y_prediction = mpg_model.predict(X_test)\n",
    "    mean_abs_errors.append(mean_absolute_error(y_test, y_prediction))\n",
    "\n",
    "# Find the best depth based on mean absolute errors\n",
    "best_depth = depths[mean_abs_errors.index(min(mean_abs_errors))]\n",
    "\n",
    "print(f\"The decision tree performs best with a depth of {best_depth}\")\n",
    "print(f\"The mean absolute errors of the different depths are: {mean_abs_errors}\")\n",
    "\n",
    "# Plotting the decision tree\n",
    "fig, ax = plt.subplots(figsize=(70, 50))\n",
    "tree.plot_tree(mpg_model, ax=ax, filled=True, feature_names=list(X_1.columns))  \n",
    "\n",
    "# Saving the plot if it doesn't already exist\n",
    "if not os.path.exists(\"Decision_Tree.png\"):\n",
    "    plt.savefig(\"Decision_Tree.png\")\n",
    "    print(\"Decision tree visualization saved as 'Decision_Tree.png'\")\n",
    "else:\n",
    "    print(\"File 'Decision_Tree.png' already exists\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f5e74-6a72-4514-97f0-646bae6093f2",
   "metadata": {},
   "source": [
    "## Final Outlook and areas for improvement.\n",
    "\n",
    "Overall I am quite satisfied with what I was able to create, I had a lot of fun going through this modeling process and learning new modules as well as sharpening my skills when it comes to known ones. I think that similar model quality could have been reached with fewer features in hindsight, to achieve this, I think that I should have first examined if there is a correlation between the other variables and mpg, if the correlation is minor in comparison to the others, then it would be fair to assume that these do not bring us that much more information when our goal is calculating the mpg. I am also a little disappointed in the depth of the tree, we are bordering on overfitting as we have 8 possible features, and we use 7 in our decision tree. The biggest improvement that I think I could make on my next go is using a random forest as my model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
